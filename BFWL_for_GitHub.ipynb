{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BI-FIDELTY WEIGHTED LEARNING FOR NEURAL NETWORKS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For details see: \n",
    " <br>De, S., Britton, J., Reynolds, M., Skinner, R., Jansen, K., & Doostan, A. (2020). \n",
    " <br>**\"On transfer learning of neural networks using bi-fidelity data for uncertainty propagation.\"**\n",
    " <br>International Journal for Uncertainty Quantification, 10(6).\n",
    " <br>Link: http://www.dl.begellhouse.com/journals/52034eb04b657aea,3673619972b2eee6,3364eea04170ec7c.html\n",
    "        \n",
    " To cite, please use the following BibTex entry:   \n",
    " @article{de2020transfer,\n",
    " <br>title={On transfer learning of neural networks using bi-fidelity data for uncertainty propagation},\n",
    " <br>author={De, Subhayan and Britton, Jolene and Reynolds, Matthew and Skinner, Ryan and Jansen, Kenneth and Doostan, Alireza},\n",
    " <br>journal={International Journal for Uncertainty Quantification},\n",
    " <br>volume={10},\n",
    " <br>number={6},\n",
    " <br>year={2020},\n",
    " <br>publisher={Begel House Inc.}\n",
    " <br>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic, WhiteKernel, RBF, Matern\n",
    "from scipy.io import loadmat  # this is the SciPy module that loads mat-files\n",
    "import scipy.io \n",
    "\n",
    "# Load the dataset\n",
    "# Coarse dataset\n",
    "mat = scipy.io.loadmat('c_c.mat')\n",
    "c_c=mat['c_c'] \n",
    "# Fine dataset\n",
    "mat = scipy.io.loadmat('c_f.mat')\n",
    "c_f=mat['c_f'] \n",
    "# Input random variables\n",
    "mat = scipy.io.loadmat('rv.mat')\n",
    "input_data=mat['rv']  \n",
    "# Spatial average of concentrations\n",
    "lf_data = np.mean(c_c,axis=1)\n",
    "hf_data = np.mean(c_f,axis=1)\n",
    "\n",
    "# For bifidelity models \n",
    "hf_train_size_bf = 20 # Number of HF datapoints used\n",
    "lf_train_size_bf = 140 # Number of LF datapoints used\n",
    "val_size_bf = 50 # Number of validation datapoints\n",
    "\n",
    "# Input and Output Dimensions\n",
    "dim_in = input_data.shape[1]          \n",
    "dim_out = 1\n",
    "\n",
    "# Activation Function\n",
    "act_func = 'ELU'\n",
    "h_size = [50, 50] # Two hidden layers with 50 neurons each\n",
    "\n",
    "#%% --------------------------------------------------------------------------------------------------------------------\n",
    "#     NN MODEL CLASS\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden = nn.ModuleList()\n",
    "        self.hidden.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            self.hidden.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden:\n",
    "            if act_func == 'ReLU':\n",
    "                x = F.relu(layer(x))\n",
    "            elif act_func == 'tanh':\n",
    "                x = F.tanh(layer(x))\n",
    "            elif act_func == 'Hardtanh':\n",
    "                x = F.hardtanh(layer(x))\n",
    "            elif act_func == 'Sigmoid':\n",
    "                x = F.sigmoid(layer(x))\n",
    "            elif act_func == 'Softsign':\n",
    "                x = F.softsign(layer(x))\n",
    "            elif act_func == 'ELU':\n",
    "                x = F.elu(layer(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: pre-training, Epoch: 0, Val Err: 1.00004\n",
      "Type: pre-training, Epoch: 6000, Val Err: 0.01189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subha\\.conda\\envs\\pytorchenv\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: soft_finetuning_bf, Epoch: 0, Val Err: 0.00555\n",
      "Type: soft_finetuning_bf, Epoch: 500, Val Err: 0.00613\n",
      "Type: pre-training, Epoch: 0, Val Err: 1.00005\n",
      "Type: pre-training, Epoch: 6000, Val Err: 0.01202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subha\\.conda\\envs\\pytorchenv\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: soft_finetuning_bf, Epoch: 0, Val Err: 0.01101\n",
      "Type: soft_finetuning_bf, Epoch: 500, Val Err: 0.01028\n"
     ]
    }
   ],
   "source": [
    "best_err_all = np.array([]) # saves best error for different randomization of datasets\n",
    "t1_start = time.process_time() \n",
    "for iseed in range(30): # for different replication of datasets\n",
    "    # For shuffling data\n",
    "    np.random.seed(iseed)\n",
    "    rand_inputs = np.random.permutation(input_data.shape[0])\n",
    "\n",
    "    #%% ----------------------------------------------------------------------------------------------------------------\n",
    "    #     DATA: BIFIDELITY MODELS\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "         \n",
    "    # Shuffle data    \n",
    "    hf_sample_set_bf = rand_inputs[:hf_train_size_bf]\n",
    "    lf_sample_set_bf = rand_inputs[hf_train_size_bf:hf_train_size_bf+lf_train_size_bf]\n",
    "    val_sample_set_bf = rand_inputs[hf_train_size_bf+lf_train_size_bf:hf_train_size_bf+lf_train_size_bf+val_size_bf]\n",
    "    \n",
    "    # HF data\n",
    "    x_bf_hfpts = input_data[hf_sample_set_bf,:]\n",
    "    y_bf_train_hfdata_hfpts = hf_data[hf_sample_set_bf]\n",
    "    y_bf_train_hfdata_lfpts = hf_data[lf_sample_set_bf]\n",
    "    \n",
    "    # LF data\n",
    "    x_bf_lfpts = input_data[lf_sample_set_bf,:]\n",
    "    y_bf_train_lfdata_lfpts = lf_data[lf_sample_set_bf]\n",
    "    \n",
    "    # Soft data\n",
    "    x_bf_softpts = np.concatenate((x_bf_lfpts, x_bf_hfpts))\n",
    "    y_bf_train_hfdata_softpts = np.concatenate((y_bf_train_hfdata_lfpts, y_bf_train_hfdata_hfpts))\n",
    "    \n",
    "    # Validation data\n",
    "    x_bf_val = input_data[val_sample_set_bf,:]\n",
    "    y_bf_train_val_hfdata = hf_data[val_sample_set_bf]\n",
    "    \n",
    "    # Write inputs as torch variables\n",
    "    x_hf_train = torch.from_numpy(x_bf_hfpts).float()\n",
    "    x_lfpretrain_train = torch.from_numpy(x_bf_lfpts).float()\n",
    "    x_soft_train = torch.from_numpy(x_bf_softpts).float()\n",
    "    x_bf_val = torch.from_numpy(x_bf_val).float()\n",
    "    \n",
    "    # Write targets as torch variables\n",
    "    y_bf_hfpts = torch.from_numpy(y_bf_train_hfdata_hfpts).float().unsqueeze(1)\n",
    "    y_bf_lfpts = torch.from_numpy(y_bf_train_lfdata_lfpts).float().unsqueeze(1)\n",
    "    \n",
    "    #%% ----------------------------------------------------------------------------------------------------------------\n",
    "    #     BIFIDELITY MODEL: PRETRAIN STUDENT NETWORK ON LF DATA\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    torch.manual_seed(100) # for reproducibility \n",
    "\n",
    "    # LF model\n",
    "    model_lfpretrain = Net(input_size=dim_in, hidden_sizes=h_size, output_size=dim_out)\n",
    "    \n",
    "    # Use Mean Squared Error as loss function\n",
    "    criterion = nn.MSELoss(size_average=False)\n",
    "    \n",
    "    # Number of epochs and iterations\n",
    "    batch_size = len(y_bf_lfpts)\n",
    "    num_iters = len(y_bf_lfpts) / batch_size\n",
    "    \n",
    "    # Determine the optimizer used\n",
    "    m = 'Adam'\n",
    "    l_rate = 1e-4*4\n",
    "    num_epochs_lfpretrain = 12000\n",
    "    if m == 'SGD':\n",
    "        optimizer = optim.SGD(model_lfpretrain.parameters(), lr=l_rate)    \n",
    "    elif m == 'SGDMomentum':\n",
    "        optimizer = optim.SGD(model_lfpretrain.parameters(), lr=l_rate, momentum=0.9)\n",
    "    elif m == 'Adam':\n",
    "        optimizer = optim.Adam(model_lfpretrain.parameters(), lr=l_rate, betas=(0.9, 0.99))\n",
    "    elif m == 'LBFGS':\n",
    "        optimizer = optim.LBFGS(model_lfpretrain.parameters(), lr=l_rate, max_iter=20, max_eval=50, \n",
    "                                tolerance_grad=1e-3, tolerance_change=1e-3, history_size=50)\n",
    "        \n",
    "    num_iters = int(num_iters)\n",
    "    err_store = np.array([])\n",
    "    \n",
    "    y_pred_val = model_lfpretrain(x_bf_val).squeeze()\n",
    "    y_pred_val = y_pred_val.data.numpy() \n",
    "    \n",
    "    # Start timer\n",
    "    start = time.time()\n",
    "    \n",
    "    # Copy model\n",
    "    best_model = copy.deepcopy(model_lfpretrain.state_dict())\n",
    "    best_err = 10000000000\n",
    "    \n",
    "    # Pretrain on data\n",
    "    #torch.manual_seed(4)\n",
    "    patience = 0 # Stopping criterion \n",
    "    data_type = 'pre-training'\n",
    "    for epoch in range(num_epochs_lfpretrain):\n",
    "    \n",
    "        for i in range(num_iters):\n",
    "            # Forward: input x and predict based on x\n",
    "            xi = x_lfpretrain_train[i*batch_size:(i+1)*batch_size]\n",
    "            yi = y_bf_lfpts[i*batch_size:(i+1)*batch_size]\n",
    "            pred_y = model_lfpretrain(xi)\n",
    "            \n",
    "            # Compute loss: compare NN output to target values y_i\n",
    "            loss = criterion(pred_y, yi)\n",
    "            \n",
    "            # Clear gradients before backward pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Backprop, compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step() \n",
    "            \n",
    "        # Predicted training and validation values\n",
    "        y_pred_train = model_lfpretrain(x_lfpretrain_train)\n",
    "        y_pred_train = y_pred_train.data.numpy()\n",
    "        y_pred_val = model_lfpretrain(x_bf_val).squeeze()\n",
    "        y_pred_val = y_pred_val.data.numpy() \n",
    "        \n",
    "        divided = 10\n",
    "        err_val = linalg.norm(y_pred_val - y_bf_train_val_hfdata)/linalg.norm(y_bf_train_val_hfdata)\n",
    "        \n",
    "        patience +=1\n",
    "        \n",
    "        if epoch % divided == 0:\n",
    "            err_store = np.append(err_store, err_val)\n",
    "        \n",
    "        if err_val < best_err:\n",
    "            best_err = err_val\n",
    "            best_model = copy.deepcopy(model_lfpretrain.state_dict()) \n",
    "            patience = 0\n",
    "            \n",
    "        # Print Results\n",
    "        if epoch % 6000 == 0:\n",
    "            print('Type: %s, Epoch: %d, Val Err: %.5f' %(data_type,epoch, err_val))\n",
    "            \n",
    "        # check patience\n",
    "        if patience > 8000:\n",
    "            break \n",
    "    \n",
    "    # Stop timer\n",
    "    time_elapsed = time.time() - start\n",
    "    \n",
    "    # Keep the best model\n",
    "    model_lfpretrain.load_state_dict(best_model)\n",
    "    torch.save(model_lfpretrain.state_dict(), 'battery_lf_pretraining_bf_model.pt')\n",
    "    \n",
    "    #%% ----------------------------------------------------------------------------------------------------------------\n",
    "    #     BIFIDELITY MODEL\n",
    "    #     TRAIN THE TEACHER WITH HF DATA\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    batch_size = 1\n",
    "    num_epochs_fwl = 1000\n",
    "    # Kernel function \n",
    "    kernel = 1.0 * RationalQuadratic(length_scale_bounds=(20,21)) + WhiteKernel(noise_level_bounds=(1e-7,1e-5)) \n",
    "    \n",
    "    # train on strong dataset\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-10).fit(x_hf_train, y_bf_hfpts)\n",
    "    y_gp_hf_mean, y_gp_hf_cov = gp.predict(x_hf_train, return_cov=True)\n",
    "    y_gp_hf_mean = y_gp_hf_mean.squeeze()\n",
    "        \n",
    "    # Get mean m and covariance matrix K for soft data\n",
    "    y_gp_soft_mean, y_gp_soft_cov = gp.predict(x_soft_train, return_cov=True)\n",
    "    y_gp_soft_mean = y_gp_soft_mean.squeeze()  \n",
    "    \n",
    "    # Teacher T(x_t) = g(m(x_t)), here g = identity\n",
    "    # \\bar{y}(x_t) = T(x_t)\n",
    "    y_pred_gp_train = y_gp_soft_mean\n",
    "    y_pred_gp_train = torch.from_numpy(y_pred_gp_train).float().reshape(-1,1)\n",
    "    \n",
    "    # Uncertainty Sigma(x_t) = h(K(x_t, x_t)), here h = I\n",
    "    sigma_soft_train = np.diag(y_gp_soft_cov)\n",
    "    \n",
    "    # Val error\n",
    "    err_gp_val = linalg.norm(y_gp_soft_mean - y_bf_train_hfdata_softpts)/linalg.norm(y_bf_train_hfdata_softpts)\n",
    "    \n",
    "    #%% ----------------------------------------------------------------------------------------------------------------\n",
    "    #     FINE-TUNE THE STUDENT NETWORK WITH SOFT DATA\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    err_fwl_train = np.zeros((len(betas), 1))\n",
    "    err_fwl_val = np.zeros((len(betas), 1))\n",
    "    y_pred_fwl_train = np.zeros((len(betas), len(y_bf_train_hfdata_softpts)))\n",
    "    y_pred_fwl_val = np.zeros((len(betas), len(y_bf_train_val_hfdata)))\n",
    "    time_fwl = np.zeros((len(betas), 1))\n",
    "    best_err_fwl = np.zeros((len(betas), 1))\n",
    "    err_store_fwl = np.zeros((len(betas), \n",
    "                              int(np.floor(num_epochs_fwl/divided))))\n",
    "    \n",
    "    beta = 0.01\n",
    "    \n",
    "    # Compute eta_2\n",
    "    eta = np.exp(-beta * sigma_soft_train) # why? aggregation\n",
    "    \n",
    "    # Load model with LF trained weight to use with HF data\n",
    "    model_fwl = Net(input_size=dim_in, hidden_sizes=h_size, output_size=dim_out)\n",
    "    model_fwl.load_state_dict(torch.load('battery_lf_pretraining_bf_model.pt'))\n",
    "    \n",
    "    # Use Mean Squared Error as loss function\n",
    "    criterion = nn.MSELoss(size_average=False)\n",
    "    num_iters = len(y_pred_gp_train)\n",
    "    # Determine the optimizer used\n",
    "    m = 'Adam'\n",
    "    l_rate = 1e-4 # learning rate\n",
    "    if m == 'SGD':\n",
    "        optimizer = optim.SGD(model_fwl.parameters(), lr=l_rate) \n",
    "    elif m == 'SGDMomentum':\n",
    "        optimizer = optim.SGD(model_fwl.parameters(), lr=l_rate, momentum=0.9, nesterov=True)\n",
    "    elif m == 'Adam':\n",
    "        optimizer = optim.Adam(model_fwl.parameters(), lr=l_rate, betas=(0.9, 0.99))\n",
    "    elif m == 'LBFGS':\n",
    "        optimizer = optim.LBFGS(model_fwl.parameters(), lr=l_rate, max_iter=200)\n",
    "        \n",
    "    data_type = 'soft_finetuning_bf'\n",
    "    best_err_fwl=10000\n",
    "    err_store_fwl = np.array([])\n",
    "    patience = 0\n",
    "    for epoch in range(num_epochs_fwl):\n",
    "    \n",
    "        for i in range(num_iters):\n",
    "            # Forward: input x and predict based on x\n",
    "            xi = x_soft_train[i*batch_size:(i+1)*batch_size]\n",
    "            yi = y_pred_gp_train[i*batch_size:(i+1)*batch_size]\n",
    "            pred_y = model_fwl(xi)                \n",
    "            \n",
    "            # Compute loss: compare NN output to target values y_i\n",
    "            loss = criterion(pred_y, yi)\n",
    "            \n",
    "            # Clear gradients before backward pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Backprop, compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.param_groups[0]['lr'] *= eta[i]\n",
    "            # perform one step of optimization\n",
    "            optimizer.step() \n",
    "            # change the learning rate back to its original value\n",
    "            optimizer.param_groups[0]['lr'] *= (1.0/eta[i])\n",
    "            \n",
    "        # Predicted training and validation values\n",
    "        y_pred_train = model_fwl(x_soft_train)\n",
    "        y_pred_train = y_pred_train.data.numpy()\n",
    "        y_pred_val = model_fwl(x_bf_val).squeeze()\n",
    "        y_pred_val = y_pred_val.data.numpy() \n",
    "        \n",
    "        divided = 10\n",
    "        err_val = linalg.norm(y_pred_val - y_bf_train_val_hfdata)/linalg.norm(y_bf_train_val_hfdata)\n",
    "        \n",
    "        patience += 1\n",
    "        \n",
    "        if epoch % divided == 0:\n",
    "            err_store_fwl = np.append(err_store_fwl, err_val)\n",
    "        \n",
    "        if err_val < best_err_fwl:\n",
    "            best_err_fwl = err_val\n",
    "            best_model = copy.deepcopy(model_fwl.state_dict())\n",
    "            patience = 0 \n",
    "            \n",
    "        # Print Results\n",
    "        if epoch % 500 == 0:\n",
    "            print('Type: %s, Epoch: %d, Val Err: %.5f' %(data_type,epoch, err_val))\n",
    "        \n",
    "        # check patience\n",
    "        if patience > 500:\n",
    "            break\n",
    "    \n",
    "    # Stop timer\n",
    "    time_elapsed = time.time() - start\n",
    "    \n",
    "    # Keep the best model\n",
    "    model_fwl.load_state_dict(best_model)\n",
    "    torch.save(model_fwl.state_dict(), 'battery_fwl_bf_model.pt')\n",
    "    \n",
    "    best_err_all = np.append(best_err_all,np.around(best_err_fwl,6))\n",
    "    \n",
    "t1_elapsed = time.process_time() - t1_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.006283, 0.010089, 0.011663, 0.012303])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_err_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
